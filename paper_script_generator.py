"""
è«–æ–‡ç´¹ä»‹ç”¨ã®å°æœ¬ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
è¤‡æ•°ã®è«–æ–‡ã‚’ç´¹ä»‹ã™ã‚‹Podcastå½¢å¼ã®å°æœ¬ã‚’ç”Ÿæˆ
"""
import os
import requests
import json
import re
from dotenv import load_dotenv
from lm_studio_utils import ensure_lm_studio_ready

load_dotenv()

LM_STUDIO_URL = os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1") + "/chat/completions"
API_KEY = os.getenv("LM_STUDIO_API_KEY", "lm-studio")
DEFAULT_MODEL = "openai/gpt-oss-20b"
SUMMARY_MAX_CHARS = int(os.getenv("PAPER_SUMMARY_MAX_CHARS", "1200"))
DIALOGUE_MAX_CHARS = int(os.getenv("PAPER_DIALOGUE_MAX_CHARS", "420"))
LM_STUDIO_TIMEOUT = int(os.getenv("LM_STUDIO_TIMEOUT", "240"))
LM_STUDIO_REWRITE_TIMEOUT = int(os.getenv("LM_STUDIO_REWRITE_TIMEOUT", "180"))
LM_STUDIO_REWRITE_BATCH_SIZE = int(os.getenv("LM_STUDIO_REWRITE_BATCH_SIZE", "5"))
LM_STUDIO_MAX_TOKENS = int(os.getenv("LM_STUDIO_MAX_TOKENS", "3200"))
DEFAULT_SPEAKER_NAME = os.getenv("VOICEVOX_SPEAKER_NAME", "é’å±±é¾æ˜Ÿ")
CJK_RANGE = r"\u3040-\u30ff\u3400-\u9fff"
SPACE_BETWEEN_CJK = re.compile(rf"(?<=[{CJK_RANGE}0-9])\s+(?=[{CJK_RANGE}0-9])")
SPACE_BETWEEN_CJK_ASCII = re.compile(rf"(?<=[{CJK_RANGE}])\s+(?=[A-Za-z0-9])")
SPACE_BETWEEN_ASCII_CJK = re.compile(rf"(?<=[A-Za-z0-9])\s+(?=[{CJK_RANGE}])")
SENTENCE_SPLIT_RE = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿ!?])")
SOFT_BREAK_CHARS = ["ã€", "ï¼Œ", ",", "ãƒ»", "ï¼", "/", " ", "ã€€", "ï¼›", ";", ":", "ï¼š"]
ASCII_LETTER_RE = re.compile(r"[A-Za-z]")
SKIP_TAG_RE = re.compile(r"<skip>.*?</skip>", flags=re.DOTALL)
ENGLISH_TOKEN_RE = re.compile(r"[A-Za-z][A-Za-z0-9+\\-./]*")

ABBREVIATION_READINGS = [
    ("fMRI", "ã‚¨ãƒ•ã‚¨ãƒ ã‚¢ãƒ¼ãƒ«ã‚¢ã‚¤"),
    ("sEEG", "ã‚¨ã‚¹ã‚¤ãƒ¼ã‚¤ãƒ¼ã‚¸ãƒ¼"),
    ("iEEG", "ã‚¢ã‚¤ã‚¤ãƒ¼ã‚¤ãƒ¼ã‚¸ãƒ¼"),
    ("EEG", "ã‚¤ãƒ¼ã‚¤ãƒ¼ã‚¸ãƒ¼"),
    ("MEG", "ã‚¨ãƒ ã‚¤ãƒ¼ã‚¸ãƒ¼"),
    ("EMG", "ã‚¤ãƒ¼ã‚¨ãƒ ã‚¸ãƒ¼"),
    ("ECG", "ã‚¤ãƒ¼ã‚·ãƒ¼ã‚¸ãƒ¼"),
    ("ERP", "ã‚¤ãƒ¼ã‚¢ãƒ¼ãƒ«ãƒ”ãƒ¼"),
    ("MRI", "ã‚¨ãƒ ã‚¢ãƒ¼ãƒ«ã‚¢ã‚¤"),
    ("PET", "ãƒ”ãƒ¼ã‚¤ãƒ¼ãƒ†ã‚£ãƒ¼"),
    ("BCI", "ãƒ“ãƒ¼ã‚·ãƒ¼ã‚¢ã‚¤"),
    ("CNN", "ã‚·ãƒ¼ã‚¨ãƒŒã‚¨ãƒŒ"),
    ("RNN", "ã‚¢ãƒ¼ãƒ«ã‚¨ãƒŒã‚¨ãƒŒ"),
    ("GRU", "ã‚¸ãƒ¼ã‚¢ãƒ¼ãƒ«ãƒ¦ãƒ¼"),
    ("LSTM", "ã‚¨ãƒ«ã‚¨ã‚¹ãƒ†ã‚£ãƒ¼ã‚¨ãƒ "),
    ("SVM", "ã‚¨ã‚¹ãƒ–ã‚¤ã‚¨ãƒ "),
    ("AI", "ã‚¨ãƒ¼ã‚¢ã‚¤"),
    ("ML", "ã‚¨ãƒ ã‚¨ãƒ«"),
    ("DL", "ãƒ‡ã‚£ãƒ¼ã‚¨ãƒ«"),
    ("AR", "ã‚¨ãƒ¼ã‚¢ãƒ¼ãƒ«"),
    ("VR", "ãƒ–ã‚¤ã‚¢ãƒ¼ãƒ«"),
]

ABBREVIATION_PATTERNS = [
    (
        re.compile(rf"(?<![A-Za-z0-9]){re.escape(abbr)}(?![A-Za-z0-9])"),
        reading,
        abbr,
    )
    for abbr, reading in ABBREVIATION_READINGS
]


def resolve_model():
    model = os.getenv("LM_STUDIO_MODEL")
    if model:
        return model

    base_url = os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
    try:
        response = requests.get(f"{base_url}/models", timeout=10)
        response.raise_for_status()
        data = response.json().get("data", [])
        for item in data:
            model_id = item.get("id")
            if model_id and "embed" not in model_id.lower():
                return model_id
        if data:
            return data[0].get("id")
    except Exception as e:
        print(f"Warning: Could not fetch LM Studio models: {e}")

    return DEFAULT_MODEL


def normalize_summary(summary):
    cleaned = " ".join((summary or "").split())
    if not cleaned:
        return "è¦ç´„ãªã—"
    if len(cleaned) > SUMMARY_MAX_CHARS:
        return cleaned[:SUMMARY_MAX_CHARS] + "..."
    return cleaned


def normalize_dialogue_text(text):
    if not text:
        return ""
    normalized = re.sub(r"\s+", " ", text).strip()
    normalized = SPACE_BETWEEN_CJK.sub("", normalized)
    normalized = SPACE_BETWEEN_CJK_ASCII.sub("", normalized)
    normalized = SPACE_BETWEEN_ASCII_CJK.sub("", normalized)
    normalized = re.sub(r"\s+([ã€ã€‚ï¼ï¼Ÿâ€¦])", r"\1", normalized)
    normalized = re.sub(r"([ã€Œã€ï¼ˆã€])\s+", r"\1", normalized)
    normalized = re.sub(r"\s+([ã€ã€ï¼‰ã€‘])", r"\1", normalized)
    return normalized


def apply_abbreviation_readings(text):
    normalized = text
    for pattern, reading, abbr in ABBREVIATION_PATTERNS:
        normalized = pattern.sub(f"{reading}ï¼ˆ<skip>{abbr}</skip>ï¼‰", normalized)
    return normalized


def replace_outside_skip(text, repl_func):
    parts = []
    last = 0
    for match in SKIP_TAG_RE.finditer(text):
        parts.append(repl_func(text[last:match.start()]))
        parts.append(match.group(0))
        last = match.end()
    parts.append(repl_func(text[last:]))
    return "".join(parts)


def replace_outside_parentheses(text, repl_func):
    parts = []
    buf = ""
    depth = 0
    for ch in text:
        if ch == "ï¼ˆ":
            if depth == 0:
                parts.append(("outside", buf))
                buf = ""
            depth += 1
            buf += ch
        elif ch == "ï¼‰":
            if depth > 0:
                depth -= 1
            buf += ch
            if depth == 0:
                parts.append(("inside", buf))
                buf = ""
        else:
            buf += ch
    if buf:
        parts.append(("outside" if depth == 0 else "inside", buf))

    rebuilt = []
    for kind, chunk in parts:
        if kind == "outside":
            rebuilt.append(repl_func(chunk))
        else:
            rebuilt.append(chunk)
    return "".join(rebuilt)


def fallback_wrap_english(text):
    def repl(match):
        token = match.group(0)
        return f"è‹±èªè¡¨è¨˜ï¼ˆ<skip>{token}</skip>ï¼‰"
    def apply_rules(segment):
        return replace_outside_parentheses(segment, lambda s: ENGLISH_TOKEN_RE.sub(repl, s))
    return replace_outside_skip(text, apply_rules)


def split_long_text(text, max_chars):
    if not text:
        return []
    text = text.strip()
    if max_chars <= 0 or len(text) <= max_chars:
        return [text]

    sentences = [s for s in SENTENCE_SPLIT_RE.split(text) if s]
    chunks = []
    current = ""

    for sentence in sentences:
        if not current:
            if len(sentence) <= max_chars:
                current = sentence
            else:
                chunks.extend(force_split(sentence, max_chars))
        else:
            if len(current) + len(sentence) <= max_chars:
                current += sentence
            else:
                chunks.append(current)
                if len(sentence) <= max_chars:
                    current = sentence
                else:
                    chunks.extend(force_split(sentence, max_chars))
                    current = ""

    if current:
        chunks.append(current)

    return [c.strip() for c in chunks if c.strip()]


def force_split(text, max_chars):
    remaining = text.strip()
    chunks = []
    while remaining and len(remaining) > max_chars:
        cut = -1
        for ch in SOFT_BREAK_CHARS:
            idx = remaining.rfind(ch, 0, max_chars + 1)
            if idx > cut:
                cut = idx
        if cut <= 0:
            cut = max_chars
        chunk = remaining[:cut].strip()
        if chunk:
            chunks.append(chunk)
        remaining = remaining[cut:].strip()
    if remaining:
        chunks.append(remaining)
    return chunks


def split_dialogue_lines(dialogue, max_chars):
    if not dialogue:
        return []
    if max_chars <= 0:
        return dialogue
    split_lines = []
    for line in dialogue:
        if not isinstance(line, dict):
            continue
        speaker = line.get("speaker") or DEFAULT_SPEAKER_NAME
        text = normalize_dialogue_text(line.get("text", ""))
        for chunk in split_long_text(text, max_chars):
            normalized_chunk = normalize_dialogue_text(chunk)
            if normalized_chunk:
                split_lines.append({"speaker": speaker, "text": normalized_chunk})
    return split_lines


def rewrite_english_dialogue(dialogue):
    targets = []
    for idx, line in enumerate(dialogue):
        text = line.get("text", "")
        text = apply_abbreviation_readings(text)
        dialogue[idx]["text"] = text
        text_no_skip = SKIP_TAG_RE.sub("", text)
        if ASCII_LETTER_RE.search(text_no_skip):
            targets.append({"index": idx, "text": text})

    if not targets:
        return dialogue

    system_prompt = """
ã‚ãªãŸã¯æ—¥æœ¬èªã®ç·¨é›†è€…ã§ã™ã€‚
ä»¥ä¸‹ã®å°è©ã«å«ã¾ã‚Œã‚‹è‹±èªãƒ»è‹±å­—ç•¥èªã‚’ã€å¿…ãšæ—¥æœ¬èªã«è¨€ã„æ›ãˆã€åŸæ–‡è‹±èªã¯ <skip>English</skip> ã§å¾Œç½®ã—ã¦ãã ã•ã„ã€‚
è¡¨ç¤ºä¸Šã¯æ‹¬å¼§æ›¸ãã«ã—ãŸã„å ´åˆã€ä¾‹ã®ã‚ˆã†ã«ã—ã¾ã™: ã€‡ã€‡ï¼ˆ<skip>Original English</skip>ï¼‰
è‹±èªã ã‘ã®æ–‡ã¯ç¦æ­¢ã§ã™ã€‚æ„å‘³ã¯å¤‰ãˆãšã€æƒ…å ±ã‚’è¿½åŠ ã—ãªã„ã§ãã ã•ã„ã€‚
ç•¥èªã¯ã‚«ã‚¿ã‚«ãƒŠèª­ã¿ï¼‹è‹±å­—ã‚’ <skip> </skip> ã§ä½µè¨˜ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ã‚¤ãƒ¼ã‚¤ãƒ¼ã‚¸ãƒ¼ï¼ˆ<skip>EEG</skip>ï¼‰ï¼‰ã€‚
"""

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    model = resolve_model()

    batch_size = max(1, LM_STUDIO_REWRITE_BATCH_SIZE)
    for start in range(0, len(targets), batch_size):
        batch = targets[start:start + batch_size]
        payload_json = json.dumps(batch, ensure_ascii=False)
        user_prompt = f"""æ¬¡ã®å°è©ã‚’ãƒ«ãƒ¼ãƒ«ã«æ²¿ã£ã¦æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚
JSONé…åˆ—ã§è¿”ã—ã€å„è¦ç´ ã¯ {{"index": æ•°å­—, "text": "ä¿®æ­£å¾Œã®å°è©"}} ã®å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚
ä¸¦ã³é †ã¯å…¥åŠ›ã¨åŒã˜ã«ã—ã¦ãã ã•ã„ã€‚

å¯¾è±¡å°è©:
{payload_json}
"""

        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.2,
            "max_tokens": 1200,
            "stream": False
        }

        try:
            response = requests.post(
                LM_STUDIO_URL,
                headers=headers,
                json=payload,
                timeout=LM_STUDIO_REWRITE_TIMEOUT
            )
            response.raise_for_status()
            result = response.json()
            content = result['choices'][0]['message']['content']
            content = content.replace("```json", "").replace("```", "").strip()
            start_idx = content.find('[')
            end_idx = content.rfind(']')
            if start_idx != -1 and end_idx != -1:
                content = content[start_idx:end_idx + 1]
            rewritten = json.loads(content)
            if isinstance(rewritten, list):
                for item in rewritten:
                    idx = item.get("index")
                    text = item.get("text")
                    if isinstance(idx, int) and 0 <= idx < len(dialogue) and isinstance(text, str):
                        dialogue[idx]["text"] = normalize_dialogue_text(text)
        except Exception as e:
            print(f"Warning: Failed to rewrite English dialogue batch: {e}")

    return dialogue


def format_date_jp(date_str):
    try:
        year, month, day = date_str.split("-")
        return f"{int(year)}å¹´{int(month)}æœˆ{int(day)}æ—¥"
    except Exception:
        return date_str


def generate_paper_script(papers, date_str=None):
    """
    è¤‡æ•°ã®è«–æ–‡æƒ…å ±ã‹ã‚‰Podcastå°æœ¬ã‚’ç”Ÿæˆ

    Args:
        papers: è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        date_str: æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆä¾‹: "2024-01-19"ï¼‰

    Returns:
        dict: å°æœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆtitle, dialogue, referencesï¼‰
    """
    if not ensure_lm_studio_ready():
        print("LM Studio is not available. Script generation aborted.")
        return None
    if date_str is None:
        from datetime import datetime
        date_str = datetime.now().strftime("%Y-%m-%d")

    jp_date_str = format_date_jp(date_str)

    # è«–æ–‡æƒ…å ±ã‚’æ•´å½¢
    papers_text = ""
    for i, paper in enumerate(papers, 1):
        summary = normalize_summary(paper.get("summary"))
        doi = paper.get("doi") or "ãªã—"
        published = paper.get("published") or "ä¸æ˜"
        papers_text += f"""
ã€è«–æ–‡{i}ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {paper.get('title', 'No Title')}
è‘—è€…: {paper.get('authors', 'Unknown')}
å‡ºå…¸: {paper.get('source', 'Unknown')}
å…¬é–‹æ—¥: {published}
URL: {paper.get('url', '')}
DOI: {doi}
è¦ç´„: {summary}
"""

    system_prompt = f"""
ã‚ãªãŸã¯äººæ°—Podcastã®æ§‹æˆä½œå®¶ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸè«–æ–‡æƒ…å ±ã‚’å…ƒã«ã€ãƒªã‚¹ãƒŠãƒ¼ãŒè¦ªã—ã¿ã‚„ã™ãã€ã‹ã¤çŸ¥çš„å¥½å¥‡å¿ƒã‚’åˆºæ¿€ã•ã‚Œã‚‹ã‚ˆã†ãªã€Œä¸€äººèªã‚Šã®å°æœ¬ã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

è©±è€…ã¯1äººã§ã™ï¼š
1. ã€Œ{DEFAULT_SPEAKER_NAME}ã€: è½ã¡ç€ã„ãŸä¸å¯§èªã§è©±ã™ãƒŠãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚

ã€æ­£ç¢ºæ€§ãƒ«ãƒ¼ãƒ«ã€‘
- å…¥åŠ›ã«ãªã„æƒ…å ±ã¯æ¨æ¸¬ã§æ–­å®šã—ãªã„
- æ•°å€¤/ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/æ‰‹æ³•å/å›ºæœ‰åè©ã¯è¦ç´„ã«ã‚ã‚‹ã‚‚ã®ã®ã¿ä½¿ç”¨ã™ã‚‹
- ä¸æ˜ãªç‚¹ã¯ã€Œè¦ç´„ã‹ã‚‰ã¯ä¸æ˜ã€ã¨æ˜è¨˜ã™ã‚‹

ã€æ•¬èªã®èª­ã¿ä¸Šã’ã€‘
- ã§ã™ãƒ»ã¾ã™èª¿ã§è‡ªç„¶ã«è©±ã™
- ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚„æ‹¬å¼§ã§æ•¬èªã‚’çœç•¥ã—ãªã„

ã€è‹±èªå‡¦ç†ãƒ«ãƒ¼ãƒ«ã€‘
- è‹±èªã¯å¿…ãšæ—¥æœ¬èªã«è¨€ã„æ›ãˆã€åŸæ–‡è‹±èªã¯ <skip>English</skip> ã¨ã—ã¦å¾Œç½®ã™ã‚‹
- è¡¨ç¤ºä¸Šã¯æ‹¬å¼§æ›¸ãã«ã—ãŸã„å ´åˆã€ä¾‹ã®ã‚ˆã†ã«ã™ã‚‹: ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ<skip>Convolutional Neural Network</skip>ï¼‰
- è‹±å­—ç•¥èªã¯ã‚«ã‚¿ã‚«ãƒŠèª­ã¿ï¼‹è‹±å­—ã‚’ <skip> </skip> ã§ä½µè¨˜ã™ã‚‹ï¼ˆä¾‹: ã‚¤ãƒ¼ã‚¤ãƒ¼ã‚¸ãƒ¼ï¼ˆ<skip>EEG</skip>ï¼‰ï¼‰
- è‹±èªã ã‘ã®æ–‡ã¯ç¦æ­¢
- æ—¥æœ¬èªè¨³ãŒé›£ã—ã„å ´åˆã¯ã€ã‚«ã‚¿ã‚«ãƒŠèª­ã¿ï¼‹<skip>è‹±èª</skip>ã«ã™ã‚‹

ã€è¡¨è¨˜ãƒ«ãƒ¼ãƒ«ã€‘
- æ—¥æœ¬èªã¯é€šå¸¸ã®è¡¨è¨˜ï¼ˆæ¼¢å­—ãƒ»ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠï¼‰ã§ã€ã²ã‚‰ãŒãªã®åˆ†ã‹ã¡æ›¸ãã¯ã—ãªã„
- ä¸è¦ãªç©ºç™½ã‚’å…¥ã‚Œãªã„
- æ—¥ä»˜ã¯ã€ŒYYYYå¹´MæœˆDæ—¥ã€å½¢å¼ã‚’ä½¿ã†
 - 1ã‚»ãƒªãƒ•ã¯æœ€å¤§{DIALOGUE_MAX_CHARS}æ–‡å­—ç¨‹åº¦ã«åã‚ã‚‹

ã€é‡è¦ã€‘å‡ºåŠ›ã¯å¿…ãšä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯(```json)ã‚„ã€å†’é ­ãƒ»æœ«å°¾ã®æŒ¨æ‹¶ã€è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚
JSONã®æ–‡æ³•ã‚¨ãƒ©ãƒ¼ï¼ˆã‚«ãƒ³ãƒæ¼ã‚Œã€é–‰ã˜ã¦ã„ãªã„å¼•ç”¨ç¬¦ãªã©ï¼‰ãŒãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

Format:
{{
  "title": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«",
  "dialogue": [
    {{"speaker": "{DEFAULT_SPEAKER_NAME}", "text": "å°è©"}}
  ]
}}
"""

    paper_count = len(papers)
    if paper_count <= 4:
        min_lines = 8
    elif paper_count <= 7:
        min_lines = 7
    else:
        min_lines = 6
    target_minutes = min(15, max(10, paper_count + 5))

    user_prompt = f"""ä»¥ä¸‹ã®è«–æ–‡æƒ…å ±ã‚’å…ƒã«ã€ç´„{target_minutes}åˆ†ç¨‹åº¦ã®è«–æ–‡ç´¹ä»‹ãƒˆãƒ¼ã‚¯å°æœ¬ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ—¥ä»˜ã¯{jp_date_str}ã§ã™ã€‚å†’é ­ã§æ—¥ä»˜ã¨ã€Œä»Šæ—¥ã®EEGè«–æ–‡ã¾ã¨ã‚ã€ã§ã‚ã‚‹ã“ã¨ã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚

å„è«–æ–‡ã«ã¤ã„ã¦ï¼š
- èƒŒæ™¯ãƒ»å…ˆè¡Œç ”ç©¶ã®ä½ç½®ã¥ã‘ï¼ˆè¦ç´„ã«ãªã„å ´åˆã¯ã€Œè¦ç´„ã‹ã‚‰ã¯ä¸æ˜ã€ã¨æ˜è¨˜ï¼‰
- ç ”ç©¶ã®ç›®çš„ãƒ»èª²é¡Œ
- æ‰‹æ³•ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»å¯¾è±¡
- ä¸»ãªçµæœã‚„ç¤ºå”†
- é™ç•Œãƒ»ä»Šå¾Œã®å±•æœ›ï¼ˆè¦ç´„ã«ãªã„å ´åˆã¯æ˜è¨˜ï¼‰

æ§‹æˆæŒ‡ç¤ºï¼š
- è«–æ–‡ã®é †ç•ªã¯å…¥åŠ›é †ã‚’å³å®ˆ
- å„è«–æ–‡ã«ã¤ãæœ€ä½{min_lines}ç™ºè©±
- 1ã‚»ãƒªãƒ•ã¯1ã€œ2æ–‡ã§ã€èª­ã¿ä¸Šã’ã‚„ã™ã„é•·ã•ã«ã™ã‚‹
- è¦ç‚¹ã®è¨€ã„æ›ãˆã‚„ç‹¬ã‚Šè¨€ã®ç¢ºèªã‚’æŒŸã¿ã€å°‘ã—é•·ã‚ã«ã™ã‚‹ï¼ˆè¦ç´„ã«ã‚ã‚‹ç¯„å›²ã§ï¼‰
- æ¨æ¸¬ã§æ–­å®šã—ãªã„

{papers_text}

å°æœ¬ã®æµã‚Œ:
1. ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ï¼ˆæ—¥ä»˜ã¨ç•ªçµ„ç´¹ä»‹ï¼‰
2. å„è«–æ–‡ã®ç´¹ä»‹ï¼ˆè¦ç‚¹ã‚’æ•´ç†ã—ãªãŒã‚‰ä¸€äººèªã‚Šã§è§£èª¬ï¼‰
3. ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆã¾ã¨ã‚ã¨æ¬¡å›äºˆå‘Šï¼‰
"""

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    model = resolve_model()
    max_tokens = min(LM_STUDIO_MAX_TOKENS, 800 + (len(papers) * 250))
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.5,
        "max_tokens": max_tokens,
        "stream": False
    }

    print(f"Generating paper review script for {len(papers)} papers...")

    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"Attempt {attempt + 1}/{max_retries}...")
            response = requests.post(LM_STUDIO_URL, headers=headers, json=payload, timeout=LM_STUDIO_TIMEOUT)
            response.raise_for_status()

            result = response.json()
            content = result['choices'][0]['message']['content']

            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            content = content.replace("```json", "").replace("```", "").strip()
            start_idx = content.find('{')
            end_idx = content.rfind('}')

            if start_idx != -1 and end_idx != -1:
                content = content[start_idx : end_idx + 1]

            script_data = json.loads(content)
            if isinstance(script_data.get("dialogue"), list):
                cleaned_dialogue = []
                for line in script_data["dialogue"]:
                    if not isinstance(line, dict):
                        continue
                    speaker = line.get("speaker") or DEFAULT_SPEAKER_NAME
                    text = normalize_dialogue_text(line.get("text", ""))
                    if text:
                        cleaned_dialogue.append({"speaker": speaker, "text": text})
                rewritten_dialogue = rewrite_english_dialogue(cleaned_dialogue)
                for line in rewritten_dialogue:
                    text = line.get("text", "")
                    text_no_skip = SKIP_TAG_RE.sub("", text)
                    if ASCII_LETTER_RE.search(text_no_skip):
                        line["text"] = normalize_dialogue_text(fallback_wrap_english(text))
                script_data["dialogue"] = split_dialogue_lines(rewritten_dialogue, DIALOGUE_MAX_CHARS)

            # å‚è€ƒæ–‡çŒ®æƒ…å ±ã‚’è¿½åŠ 
            script_data['references'] = []
            for paper in papers:
                ref = {
                    'title': paper['title'],
                    'authors': paper['authors'],
                    'url': paper['url'],
                    'doi': paper.get('doi', ''),
                    'source': paper['source'],
                    'published': paper['published']
                }
                script_data['references'].append(ref)

            script_data['date'] = date_str

            return script_data

        except json.JSONDecodeError:
            print("Error: LM Studio response was not valid JSON.")
            if attempt < max_retries - 1:
                print("Retrying...")
            else:
                print("Raw response:", content[:500])
                return None
        except Exception as e:
            print(f"Error communicating with LM Studio: {e}")
            if attempt < max_retries - 1:
                print("Retrying...")
            else:
                return None

    return None


def format_description(script_data):
    """
    YouTubeæ¦‚è¦æ¬„ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    """
    date_str = script_data.get('date', '')
    references = script_data.get('references', [])

    description = f"""ã€{date_str}ã€‘EEGã®è«–æ–‡ã¾ã¨ã‚

æœ¬æ—¥ã®EEGãƒ»è„³æ³¢é–¢é€£è«–æ–‡ã‚’ã€{DEFAULT_SPEAKER_NAME}ãŒåˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¾ã™ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š å‚è€ƒè«–æ–‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    for i, ref in enumerate(references, 1):
        description += f"""
ã€{i}ã€‘{ref['title']}
è‘—è€…: {ref['authors']}
å‡ºå…¸: {ref['source']}
URL: {ref['url']}
"""
        if ref.get('doi'):
            description += f"DOI: {ref['doi']}\n"

    description += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ™ï¸ å‡ºæ¼”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{DEFAULT_SPEAKER_NAME} (VOICEVOX)

#EEG #è„³æ³¢ #è«–æ–‡ç´¹ä»‹ #{DEFAULT_SPEAKER_NAME}
"""

    return description


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    test_papers = [
        {
            'title': 'Deep Learning for EEG-based Emotion Recognition',
            'authors': 'John Smith, Jane Doe',
            'source': 'arXiv',
            'url': 'https://arxiv.org/abs/2401.00001',
            'doi': '',
            'summary': 'This paper proposes a novel deep learning approach for emotion recognition using EEG signals...',
            'published': '2024-01-19'
        }
    ]

    script = generate_paper_script(test_papers, "2024-01-19")
    if script:
        print(json.dumps(script, indent=2, ensure_ascii=False))
        print("\n--- Description ---")
        print(format_description(script))
